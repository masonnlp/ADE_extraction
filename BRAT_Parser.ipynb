{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a935b-4b8d-4242-b6a5-821feb6307f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip install spacy\n",
    "!pip install numpy\n",
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "!pip list\n",
    "\"\"\"\n",
    "!pip install scispacy\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.3/en_core_sci_md-0.5.3.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3746d2-9719-4b02-a4b7-2bade623fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "#import en_core_web_sm\n",
    "from spacy.lang.en import English\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea1f4b-9c92-4d90-a653-bfed5cd9e96e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Retrieve all relations present in a file\n",
    "def file_reader(filename):\n",
    "\n",
    "    #Get number of rows in file:\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t')\n",
    "        row_count = sum(1 for row in reader)  # fileObject is your csv.reader\n",
    "        csvfile.close()\n",
    "        #import csv\n",
    "\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t')\n",
    "\n",
    "        num_ADE_Relations = 0\n",
    "\n",
    "        T_list = []\n",
    "        E_list = []\n",
    "        R_list = []\n",
    "        for row in reader:\n",
    "            firstWord = row[0]\n",
    "            firstLetter = firstWord[0]\n",
    "            if(firstLetter == \"T\"):\n",
    "                tag = row[0]\n",
    "                value = row[2]\n",
    "\n",
    "                valueTypeAndSpan = row[1].split()\n",
    "                valueType = valueTypeAndSpan[0]\n",
    "                start_span = valueTypeAndSpan[1]\n",
    "                end_span = valueTypeAndSpan[2]\n",
    "\n",
    "                tempDict = {\"Tag\": tag, \"Value\": value, \"Start_Span\": start_span, \"End_Span\": end_span}\n",
    "                T_list.append(tempDict)\n",
    "\n",
    "            elif(firstLetter == \"E\"):\n",
    "                tag = row[0]\n",
    "                value = row[1]\n",
    "\n",
    "                tempDict = {\"Tag\": tag, \"Value\": value}\n",
    "                E_list.append(tempDict)\n",
    "\n",
    "            elif(firstLetter == \"R\"):\n",
    "                tag = row[0]\n",
    "\n",
    "                relTypeandArgs = row[1].split()\n",
    "                relationType = relTypeandArgs[0]\n",
    "                arg1 = relTypeandArgs[1]\n",
    "                arg2 = relTypeandArgs[2]\n",
    "                if(relationType == 'ADE'):\n",
    "                    num_ADE_Relations+=1\n",
    "                tempDict = {\"Tag\": tag, \"relationType\": relationType, \"arg1\": arg1, \"arg2\":arg2}\n",
    "                R_list.append(tempDict)\n",
    "            else:\n",
    "                print(\"Something went wrong\")\n",
    "    #final list is a list of lists; a list of dictionaries, where each dictionary represents one brat annotation (E.g. E or T or R)\n",
    "    #Plus statistics\n",
    "    num_E_tags = len(E_list)\n",
    "    num_T_tags = len(T_list)\n",
    "    num_R_tags = len(R_list)\n",
    "    stats = {\"num_E_tags\":num_E_tags, \"num_T_tags\":num_T_tags, \"num_R_tags\":num_R_tags,\"num_ADE_Relations\":num_ADE_Relations}\n",
    "\n",
    "    file_dict = {\"T_list\":T_list, \"E_list\":E_list, \"R_list\":R_list, \"stats\":stats}\n",
    "    #print(finalList)\n",
    "    return(file_dict)\n",
    "\n",
    "\n",
    "\n",
    "#extract the spans for each ADE relation\n",
    "def get_ADE_spans(finalList : list[dict]):\n",
    "    T_list = finalList.get(\"T_list\")\n",
    "    E_list = finalList.get(\"E_list\")\n",
    "    R_list = finalList.get(\"R_list\")\n",
    "\n",
    "    ADE_spans = []\n",
    "    for relation in R_list:\n",
    "        if(relation.get(\"relationType\") == \"ADE\"):\n",
    "            #these args are E tags\n",
    "            arg1 = relation.get(\"arg1\").split(\":\")[1]\n",
    "            arg2 = relation.get(\"arg2\").split(\":\")[1]\n",
    "            for event in E_list:\n",
    "                #if(event.get(\"Tag\") == arg1 or event.get(\"Tag\") == arg2):\n",
    "                if(event.get(\"Tag\") == arg1):\n",
    "                    pattern = r'T\\d{1,3}'\n",
    "                    matches = re.findall(pattern, event.get(\"Value\"))\n",
    "                    if(len(matches) > 0):\n",
    "                        T = matches[0]\n",
    "                        for entry in T_list:\n",
    "                            if(entry.get(\"Tag\") == T):\n",
    "                                #print(entry.get(\"Value\"))\n",
    "                                #print(entry.get(\"Start_Span\"))\n",
    "                                #print(entry.get(\"End_Span\"))\n",
    "                                tempDict = {\"Start_Span\": int(entry.get(\"Start_Span\")), \"End_Span\": int(entry.get(\"End_Span\"))}\n",
    "                                ADE_spans.append(tempDict)\n",
    "    return(ADE_spans)\n",
    "\n",
    "\n",
    "\n",
    "#Use Spacy to tokenize the file\n",
    "def tokenize_file(text_file_name : str, spacy_tokenizer_name : str):\n",
    "    \"\"\"\n",
    "    text_file_name : the filename of the file being tokenized as a string without the .txt suffix\n",
    "    spacy_tokenizer_name: the name of the spacy tokenizer being used (e.g. \"en_core_web_sm\") \n",
    "    \"\"\"\n",
    "    import scispacy\n",
    "    import spacy\n",
    "\n",
    "    nlp = spacy.load(spacy_tokenizer_name)\n",
    "    #nlp = en_core_sci_md.load()\n",
    "    #nlp = English()\n",
    "    tokenArr = []\n",
    "    tokenStartArr = []\n",
    "    tokenEndArr = []\n",
    "\n",
    "    with open(text_file_name+\".txt\", 'r', encoding='utf-8', newline='\\n') as text_file:\n",
    "        doc = nlp(text_file.read())\n",
    "        sents = list(doc.sents)\n",
    "\n",
    "        for j in sents:\n",
    "            tokenArr.append(j.text_with_ws)\n",
    "            tokenStartArr.append(j.start_char)\n",
    "            tokenEndArr.append(j.end_char)\n",
    "        \n",
    "        d = np.array([tokenArr, tokenStartArr, tokenEndArr], dtype='object')\n",
    "    #each entry in newArr represnets a sentence; for i in newArr, i[0] is the entire doc raw string, i[1] is an array of all original span starts, i[2] is an array of all original span ends\n",
    "    return(d)\n",
    "\n",
    "\n",
    "\n",
    "#Put the ADE spans in order, numerically, from lowest to highest\n",
    "def order_list(ADE_spans):\n",
    "    temp = {frozenset(item.items()):item for item in ADE_spans}.values()\n",
    "    temp = sorted(temp, key=lambda d: d['Start_Span']) \n",
    "    return(temp)\n",
    " \n",
    "#validates retrieved strings from get_strings\n",
    "#checks if there are any elements in the original spacy_arr that aren't present in the ADEstrings or no ADEstrings\n",
    "#checks if there are any elements that are present in both ADE strings and noADE strings\n",
    "#checks if the length of ADE strings + noADE strings adds up to the length of the spacy_arr\n",
    "def validate_get_strings(spacy_arr, ADE_spans, ADE_list, noADE_list):\n",
    "    ADE_strings = []\n",
    "    for i in range(len(ADE_list)):\n",
    "        ADE_strings.append(ADE_list[i].get(\"string\"))\n",
    "        \n",
    "    noADE_strings = []\n",
    "    for i in range(len(noADE_list)):\n",
    "        noADE_strings.append(noADE_list[i].get(\"string\"))\n",
    "        \n",
    "    setADE = set(ADE_strings)\n",
    "    setNoADE = set(noADE_strings)\n",
    "    setADE.update(setNoADE)\n",
    "    setADE = list(setADE)\n",
    "    for i in range(len((setADE))):\n",
    "        setADE[i] = str(setADE[i]).strip()\n",
    "\n",
    "    for i in range(len(spacy_arr[0])):\n",
    "        spacy_arr[0][i] = str(spacy_arr[0][i]).strip()\n",
    "\n",
    "    setAll = set(spacy_arr[0])\n",
    "    setADE = set(setADE)\n",
    "    temp = (setAll ^ setADE)\n",
    "    \n",
    "    if(len(temp) > 0):\n",
    "        print(\"===================\")\n",
    "        print(\"Items in the original set that were not detected as ADE or noADE: \")\n",
    "        print(len(temp))\n",
    "\n",
    "        #original = ADE_strings[1]\n",
    "        #print(\"HERE\")\n",
    "        #print(original)\n",
    "        #print(\"HERE\")\n",
    "        #new = temp.pop()\n",
    "\n",
    "        #print(\"Length of Original: \" + str(len(original)))\n",
    "        #print(\"Length of new: \"+  str(len(new)))\n",
    "\n",
    "\n",
    "        for i in temp:\n",
    "            print('\\n>>>>>')\n",
    "            print(i)\n",
    "        #for j in setAll:\n",
    "        #    print('\\n--------')\n",
    "        #    print(j)\n",
    "    setADE = set(ADE_strings)\n",
    "    setNoADE = set(noADE_strings)\n",
    "    temp = (setADE.intersection(setNoADE))\n",
    "    if(len(temp) > 0):\n",
    "        print(\"===================\")\n",
    "        print(\"Items that are in both ADE and noADE sets:\")\n",
    "        print(len(temp))\n",
    "        for i in temp:\n",
    "            print('\\n>>>>>')\n",
    "            print(i)\n",
    "\n",
    "            \n",
    "            \n",
    "#uses ADE spans to retrive full tokens from spacy tokenized array\n",
    "def get_strings(spacy_arr, ADE_spans, debug):\n",
    "  #first, order the ADE spans array by start span\n",
    "  #next, iterate through each elemnt in the spacy_arr (3 parallel arrays)\n",
    "  #given the ordered arr ADE_spans, maintain a counter (i=0)\n",
    "  #for the ith ADE span, check if the start span occurs inside the current spacy_arr element.\n",
    "  #If the ADE span occurs inside the current lement:\n",
    "    #if the end span occurs inside the current element, add the current element to the list of ADE spans, increment i, do not increment spacy_iterator (in case there is overlap between ADE elements)\n",
    "    #if the end span does not occur inside the current element, check if there is a next element\n",
    "      #if yes, combine the current elemnt and the next elemnt and add to the list of ADE spans. Increment i, do not increment spacy_iterator.\n",
    "  #If the ADE span does not occur inside the current element, do not increment i, increment spacy_iterator.\n",
    "    ADE_spans_counter = 0\n",
    "    ADE_strings = []\n",
    "    noADE_strings = []\n",
    "\n",
    "    ADE_spans_in_same_token = 0\n",
    "    \n",
    "    multi_jaunt_counter = 0\n",
    "    \n",
    "    i = 0\n",
    "    loop = 0\n",
    "    while i < (len(spacy_arr[1])):\n",
    "        \n",
    "        if(ADE_spans_counter == len(ADE_spans)):\n",
    "            #if all ADE strings have been fetched, label the remaining as noADE\n",
    "            z = i\n",
    "            while(z < len(spacy_arr[1])):\n",
    "                noADE_dict = {\"string\":str(spacy_arr[0][z]), \"start_span\":spacy_arr[1][z], \"end_span\":spacy_arr[2][z]}\n",
    "                noADE_strings.append(noADE_dict)\n",
    "                #print(noADE_dict)\n",
    "                z+=1\n",
    "            break\n",
    "\n",
    "        #spacy_arr[0][i] is the documents ith token\n",
    "        #spacy_arr[1][i] is the documents ith start span\n",
    "        #spacy_arr[2][i] is the documents ith start span\n",
    "\n",
    "        ADE_start_span = int(ADE_spans[ADE_spans_counter].get(\"Start_Span\"))\n",
    "        ADE_end_span = int(ADE_spans[ADE_spans_counter].get(\"End_Span\"))\n",
    "\n",
    "        doc_start_span = spacy_arr[1][i]\n",
    "        doc_end_span = spacy_arr[2][i]\n",
    "\n",
    "        \n",
    "        #print(\"Doc i\")\n",
    "        #print(i)\n",
    "        if(debug):\n",
    "            print(\"\\n\\nADE Start and end:\")\n",
    "            print(ADE_start_span)\n",
    "            print(ADE_end_span)\n",
    "            print(\"Doc start and end:\")\n",
    "            print(doc_start_span)\n",
    "            print(doc_end_span)\n",
    "            print(spacy_arr[0][i])\n",
    "        \n",
    "        #print(len(spacy_arr))\n",
    "        #print(len(spacy_arr[0][0]))\n",
    "        #print((spacy_arr[1][i]))\n",
    "        #print((spacy_arr[2][i]))\n",
    "        #print(spacy_arr[0][i])\n",
    "        #print(spacy_arr[0][0])\n",
    "\n",
    "        if(ADE_start_span >= doc_start_span and ADE_start_span <= doc_end_span):\n",
    "            \n",
    "            #if the current ADE span DOES end in the current doc span, then:\n",
    "            ###Add the current doc span, move to the next ADE span\n",
    "            if(ADE_end_span <= doc_end_span and ADE_end_span >= doc_start_span):\n",
    "                #print(spacy_arr[0][i+1])\n",
    "                ADE_dict = {\"string\":str(spacy_arr[0][i]), \"doc_start_span\":spacy_arr[1][i], \"doc_end_span\":spacy_arr[2][i]}\n",
    "                ADE_strings.append(ADE_dict)\n",
    "                ADE_spans_counter+=1\n",
    "                if(debug):\n",
    "                    print(\"Added ADE span due to beginning and ending in same token\")\n",
    "                #print(ADE_strings)\n",
    "                #print('\\n======')                \n",
    "                #if the next ADE Span also occurs inside this doc token \n",
    "                ###then, add this token, incremeent ADE span, and increment the trakcer for number of ADE spans in the same token.\n",
    "                while(True):\n",
    "                    #If there are remaining ADE spans\n",
    "                    if(ADE_spans_counter < len(ADE_spans)):\n",
    "                        ADE_start_span = int(ADE_spans[ADE_spans_counter].get(\"Start_Span\"))\n",
    "                        ADE_end_span = int(ADE_spans[ADE_spans_counter].get(\"End_Span\"))\n",
    "                        #If the next ADE span occurs in the same token, add it \n",
    "                        if(ADE_end_span <= doc_end_span and ADE_end_span >= doc_start_span):\n",
    "                            ADE_spans_in_same_token+=1 \n",
    "                            ADE_spans_counter +=1\n",
    "                            ADE_strings.append(ADE_dict)\n",
    "                            if(debug):\n",
    "                                print(\"Added additional ADE span due to ending in the same token\")\n",
    "                            #print(ADE_strings)\n",
    "                            #print('\\n======')                \n",
    "                            \n",
    "                        #If the next span does not occur in the same token, move on\n",
    "                        else:\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "                i+=1\n",
    "                continue\n",
    "\n",
    "            #if the current ADE span DOES NOT end in the current doc span\n",
    "            ###Add each doc span and increment until the current ADE span ends. \n",
    "            elif(ADE_end_span > doc_end_span):\n",
    "                temp = ADE_spans_counter\n",
    "                multi_jaunt_counter+=1\n",
    "\n",
    "                while(True):\n",
    "\n",
    "                    ADE_start_span = int(ADE_spans[ADE_spans_counter].get(\"Start_Span\"))\n",
    "                    ADE_end_span = int(ADE_spans[ADE_spans_counter].get(\"End_Span\"))\n",
    "                    ADE_string = str(spacy_arr[0][i])\n",
    "\n",
    "                    doc_start_span = spacy_arr[1][i]\n",
    "                    doc_end_span = spacy_arr[2][i]\n",
    "                    ADE_dict = {\"string\":ADE_string, \"doc_start_span\":doc_start_span, \"doc_end_span\":doc_end_span}\n",
    "                    ADE_strings.append(ADE_dict)\n",
    "                    if(debug):\n",
    "                        print(\"Added ADE span due to NOT ending in same span\")\n",
    "                        \n",
    "                        print(\"doc\")\n",
    "                        print(doc_start_span)\n",
    "                        print(doc_end_span)\n",
    "                        print(\"ADE\")\n",
    "                        print(ADE_start_span)\n",
    "                        print(ADE_end_span)\n",
    "                        \n",
    "                    #print(ADE_string)\n",
    "                                 \n",
    "                   \n",
    "                    i+=1\n",
    "                    loop+=1\n",
    "                    doc_start_span = spacy_arr[1][i]\n",
    "                    doc_end_span = spacy_arr[2][i]\n",
    "\n",
    "                    if(ADE_end_span <= doc_end_span and ADE_end_span >= doc_start_span):\n",
    "                        ADE_spans_counter += 1\n",
    "                        \n",
    "                        #print(ADE_strings)\n",
    "                        #print('\\n======')   \n",
    "                        #print(\"break\")\n",
    "                        \n",
    "                        break\n",
    "                #if the loop exits without finding a match; if the end span isnt found\n",
    "                if(ADE_spans_counter == temp):\n",
    "                    print(\"Error with a multi-token ADE span\")\n",
    "                    exit()\n",
    "                else:\n",
    "                  #if the while loop exits by finding a match\n",
    "                    continue\n",
    "                    \n",
    "        #if the current span is not an ADE span, add it as a noADE span\n",
    "        else:\n",
    "            noADE_dict = {\"string\":str(spacy_arr[0][i]), \"doc_start_span\":spacy_arr[1][i], \"doc_end_span\":spacy_arr[2][i]}\n",
    "            noADE_strings.append(noADE_dict)\n",
    "        i+=1\n",
    "\n",
    "    #if the amount of ADE spans collected is not the same as the amount of ADE R tags in the original file\n",
    "    if(len(ADE_spans) != len(ADE_strings)):\n",
    "        #if the amount of ADE spans collected is not the same as the amount of ADE spans found oriignally \n",
    "        if(len(ADE_spans) != ADE_spans_counter):\n",
    "            print(\"Error!\") \n",
    "    \n",
    "    if(debug):\n",
    "        print(\"==============\")\n",
    "        print(\"ADE Strings:\")\n",
    "        for k in ADE_strings:\n",
    "            string = k.get(\"string\")\n",
    "            print(\"\\n\\nelement length:\")\n",
    "            print(len(string))\n",
    "            print(\"\\nelement:\")\n",
    "            print(string)\n",
    "        print(\"==============\")\n",
    "        print(\"Non-ADE Strings:\")\n",
    "        for k in noADE_strings:\n",
    "            string = k.get(\"string\")\n",
    "            print(\"\\n\\nelement length:\")\n",
    "            print(len(string))\n",
    "            print(\"\\nelement:\")\n",
    "            print(string)\n",
    "\n",
    "        print(\"Total Strings:\")\n",
    "        print(len(spacy_arr[1]))\n",
    "        print(\"Total ADE strings collected: \")\n",
    "        print((len(ADE_strings)))\n",
    "        print(\"Total nonADE strings collected: \")\n",
    "        print(len(noADE_strings))\n",
    "        print(\"Total original ADE Relations: \")\n",
    "        print(len(ADE_spans))\n",
    "        print(\"Loops:\")\n",
    "        print(loop)\n",
    "    \n",
    "    validate_get_strings(spacy_arr, ADE_spans, ADE_strings, noADE_strings)\n",
    "    print(multi_jaunt_counter)\n",
    "    return({\"ADE_strings\":ADE_strings, \"noADE_strings\":noADE_strings, \"num_Multi_Token_ADE_Relations\":loop})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4df28-d64c-4528-b2a6-014eced7cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BRAT_parse(filename: str, debug : bool, spacy_tokenizer_name : str):\n",
    "    \"\"\"\n",
    "    filename: absolute filepath to the ann/txt filepair to parse, sans any suffix (.txt or .ann)\n",
    "    debug: If true, prints status over execution. \n",
    "    \"\"\"\n",
    "    \n",
    "    file_dict = file_reader(filename+\".ann\")\n",
    "\n",
    "    ADE_Spans = get_ADE_spans(file_dict)\n",
    "    ADE_Spans = order_list(ADE_Spans)\n",
    "    #print(\"ADE Spans:\")\n",
    "    #print(ADE_Spans)\n",
    "    spacy_arr = []\n",
    "    spacy_arr = tokenize_file(filename, spacy_tokenizer_name)\n",
    "    \n",
    "    ADE_noADE_dict = get_strings(spacy_arr, ADE_Spans, debug)\n",
    "\n",
    "    #ADE_strings = ADE_noADE_dict.get(\"ADE_strings\")\n",
    "    #noADE_strings = ADE_noADE_dict.get(\"noADE_strings\")\n",
    "\n",
    "    return(ADE_noADE_dict)\n",
    "\n",
    "def export_data(data, path):\n",
    "    f = open(path+\".pkl\",'w')\n",
    "    f.close()\n",
    "    f = open(path+\".pkl\", \"wb\")\n",
    "    pickle.dump(data, f)\n",
    "    f.close()\n",
    "\n",
    "def parse_files(directory, foldername, train_or_validation, debug, spacy_tokenizer_name : str):\n",
    "    #directory should be same as parse_folders\n",
    "    #foldername is the name of specific folder being parsed (e.g. physician)\n",
    "    #train_or_validation should be \"train\" or \"dev\"\n",
    "    importPath = directory+foldername\n",
    "    ADE_noADE_dict_list = []\n",
    "    loop_counter = 0\n",
    "    for file in os.listdir(importPath):\n",
    "        filenameArr = file.split('.')\n",
    "        if(len(filenameArr) > 1):\n",
    "            if(filenameArr[1] == 'ann'):\n",
    "                print(filenameArr[0])\n",
    "                parse_result = BRAT_parse(importPath+'/'+filenameArr[0], debug, spacy_tokenizer_name)\n",
    "                loop_counter += (parse_result.get(\"num_Multi_Token_ADE_Relations\"))\n",
    "                del parse_result['num_Multi_Token_ADE_Relations']\n",
    "                ADE_noADE_dict_list.append(parse_result)\n",
    "                                            \n",
    "                #print(loop_counter)\n",
    "    exportPath = (directory.replace(train_or_validation,''))+'processed_data/'+train_or_validation+'3_' + foldername +'_' + spacy_tokenizer_name\n",
    "    export_data([ADE_noADE_dict_list, loop_counter, foldername], exportPath)\n",
    "    print(\"Total loops in \"+ foldername+\": \"+ str(loop_counter))\n",
    "    return(ADE_noADE_dict_list)\n",
    "\n",
    "        \n",
    "    \n",
    "#input: a filepath to a folder entitled /train containing named folders (e.g. \"Physician\") containing ann txt filepairs\n",
    "#output: a folder at the same filepath (same level as /train) named processed_data containing named folders (e.g. \"Physician\")\n",
    "#containing pkl files containig a python list of shape: [ {ADE_strings: [\"blah blah\"], noADE_strings: [\"blah blah\"] } ]\n",
    "def parse_folders(directory, train_or_validation, debug, spacy_tokenizer_name : str):\n",
    "    #Directory should be the path to the folder that contains the folders with txt and ann files (e.g. \"physiciain\" etc.)\n",
    "    #train_or_validation should be \"train\" or \"dev\". This string is used to remove the approrpiate string from the filepath where the folders are eventaully sacved.\n",
    "    final_dict = {}\n",
    "\n",
    "    for foldername in os.listdir(directory +train_or_validation+'/'):\n",
    "        print('\\n=============\\nFolder name:')\n",
    "        print(foldername)\n",
    "        ADE_noADE_dict_list = parse_files(directory +train_or_validation+'/', foldername, train_or_validation, debug, spacy_tokenizer_name)\n",
    "        \n",
    "        \"\"\"\n",
    "        print('\\n=============\\nAll_files_Dict:')\n",
    "        print(file_dict_list)\n",
    "        print('\\n==================\\n')\n",
    "        \"\"\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc45124-43d3-438b-b651-41422ac919a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(debug: bool, spacy_tokenizer_name : str):\n",
    "    parse_folders('#should equal a local folder', 'train', debug, spacy_tokenizer_name)\n",
    "    parse_folders('#should equal a local folder', 'test', debug, spacy_tokenizer_name)\n",
    "\n",
    "#main(False, \"en_core_web_sm\")\n",
    "main(False, \"en_core_sci_md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
