{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fb5b6-b636-4fa1-839d-7dbae89d3ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface-hub\n",
    "!pip install transformers==4.36\n",
    "!pip install datasets evaluate\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7989a2-ca48-4d12-96cd-9fb284d44d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def import_data(importDirectory, train_or_valid, spacy_tokenizer_name):\n",
    "    \n",
    "    export_list = []\n",
    "    for file in os.listdir(importDirectory):\n",
    "        filenameArr = file.split('.')\n",
    "        filenameArr2 = file.split('_')\n",
    "        if(len(filenameArr) > 1):\n",
    "            if(filenameArr2[0] == train_or_valid):\n",
    "                if(spacy_tokenizer_name in file):\n",
    "                    if(filenameArr[1] == 'pkl'):\n",
    "                        with open(importDirectory+'/'+file, 'rb') as f:\n",
    "                            print(importDirectory+'/'+file)\n",
    "\n",
    "                            data = pickle.load(f)\n",
    "                            f.close()\n",
    "                            #print(importDirectory)\n",
    "                            export_list.append({\"data\":data, \"foldername\":filenameArr[0]})\n",
    "    return(export_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb901a94-f29c-42ac-ba8e-0103727ac3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_path = #should equal the path to the folder containing the output of BRAT_Parser\n",
    "\n",
    "#Data is a list with a length equal to the amount of folders that were iterated through \n",
    "#data[n] is a list of length x. X is the number of files presnet in the nth folder. \n",
    "#data[n][x] is a dictionary, with keys 'ADE_strings', 'noADE_strings', 'num_Multi_Token_ADE_Relatons\n",
    "\n",
    "\n",
    "#Will work with data via Hugging Face Datasets library\n",
    "#Should be a dataset dict: {\"train\": [\"string\", \"label\", \"idx\"], \"verification\":[\"string\", \"label\", \"idx\"], \"test\":[...\n",
    "#First, cast it to a pandas df of shape [\"label\"][\"string\" ....] \n",
    "def load_data(import_path, train_or_valid, spacy_tokenizer_name):\n",
    "    data = import_data(import_path, train_or_valid, spacy_tokenizer_name)\n",
    "        \n",
    "    dataset = pd.DataFrame({})\n",
    "    ADE_Strings = [] \n",
    "    noADE_Strings = []\n",
    "    filenames = []\n",
    "    for i in data:\n",
    "        foldername = i['foldername'].split('_')[1]\n",
    "        for j in i['data'][0]:\n",
    "            #temp = j[0]\n",
    "            for k in j.get('ADE_strings'):\n",
    "                ADE_Strings.append(k.get('string'))\n",
    "                filenames.append(foldername)\n",
    "            for k in j.get('noADE_strings'):\n",
    "                noADE_Strings.append(k.get('string'))\n",
    "                filenames.append(foldername)\n",
    "\n",
    "    dataset['string'] = ADE_Strings + noADE_Strings\n",
    "    dataset['filename'] = filenames\n",
    "    \n",
    "    #Set to 1 because they are ADE\n",
    "    dataset.loc[0:len(ADE_Strings)-1, ('label')] = 1\n",
    "    #Set rest to 0 because they are not ADE\n",
    "    dataset.loc[len(ADE_Strings):len(dataset['string']), ('label')] = 0\n",
    "    return(dataset)\n",
    "\n",
    "#note: train2/dev2 names represent files that only treat the Problem events in ADE tags as ADE sentences, not both Problem AND Drug\n",
    "\n",
    "train_dataframe = load_data(import_path, 'train3', \"en_core_sci_md\")\n",
    "train_dataframe['label'] = train_dataframe['label'].astype(int)\n",
    "train_dataframe['Row_Number'] = train_dataframe.reset_index().index\n",
    "\n",
    "valid_dataframe = load_data(import_path, 'test3', \"en_core_sci_md\")\n",
    "#Without this line, will result in dimesnioanl mismatch: https://discuss.huggingface.co/t/valueerror-target-size-torch-size-8-must-be-the-same-as-input-size-torch-size-8-8/12133/2\n",
    "valid_dataframe['label'] = valid_dataframe['label'].astype(int)\n",
    "valid_dataframe['Row_Number'] = valid_dataframe.reset_index().index\n",
    "#print(type(train_dataframe['label'][0]))\n",
    "display(train_dataframe)\n",
    "display(valid_dataframe)\n",
    "\n",
    "print(\"Total length of train dataframe:\")\n",
    "print(len(train_dataframe))\n",
    "print(\"Length of class 1, ADE\")\n",
    "print(train_dataframe['label'].value_counts().get(1))\n",
    "print(\"Length of  class 2, noADE\")\n",
    "print(train_dataframe['label'].value_counts().get(0))\n",
    "\n",
    "\n",
    "print(\"Total length of test dataframe:\")\n",
    "print(len(valid_dataframe))\n",
    "print(\"Length of class 1, ADE\")\n",
    "print(valid_dataframe['label'].value_counts().get(1))\n",
    "print(\"Length of  class 2, noADE\")\n",
    "print(valid_dataframe['label'].value_counts().get(0))\n",
    "\n",
    "print(\"\\n\\nADE Example\")\n",
    "print(train_dataframe.iloc[1, 0])\n",
    "print('--------')\n",
    "print(\"noADE Example\")\n",
    "print(train_dataframe.iloc[16244, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f54087-bcbb-436d-8e59-ae5b65e08e47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#code to get counts for classes:\n",
    "print(\"Total length of train dataframe\")\n",
    "print(len(train_dataframe))\n",
    "print(\"Total length of ADE sentences in train dataframe\")\n",
    "df = train_dataframe[train_dataframe.label == 1]\n",
    "print(len(df))\n",
    "print(\"Total length of noADE sentences in train dataframe\")\n",
    "df = train_dataframe[train_dataframe.label == 0]\n",
    "print(len(df))\n",
    "print(\"----------------\")\n",
    "print(\"Total length of test dataframe\")\n",
    "print(len(valid_dataframe))\n",
    "print(\"Total length of ADE sentences in test dataframe\")\n",
    "df = valid_dataframe[valid_dataframe.label == 1]\n",
    "print(len(df))\n",
    "print(\"Total length of noADE sentences in test dataframe\")\n",
    "df = valid_dataframe[valid_dataframe.label == 0]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10996859-cc92-42f6-91f2-53c16cb3a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba92fc2-d6dc-4a4e-9109-876672b906a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_path = #path to a locally saved instance of T5-large\n",
    "from transformers import AutoTokenizer, AutoModel, DataCollatorForLanguageModeling, T5ForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "#Using AutoModel results in the error \"forward got unexpected kwarg: labels\". \n",
    "#The generic BERT does not expect labels in the forward method (https://github.com/huggingface/transformers/blob/7d9a33fb5cf40a87ff7fa9b4b8556b9bd4760461/src/transformers/models/bert/modeling_bert.py#L189)\n",
    "#Bert for seq class should\n",
    "model = T5ForSequenceClassification.from_pretrained(#model_path, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe126bf4-44d4-411c-b685-bc6cf423ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"string\"], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_dataframe)\n",
    "valid_dataset = Dataset.from_pandas(valid_dataframe)\n",
    "\n",
    "tokenized_train_datasets = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_valid_datasets = valid_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f18e61-5e10-4cf6-bb22-f3a517a076ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets = tokenized_train_datasets.remove_columns([\"string\"])\n",
    "tokenized_train_datasets = tokenized_train_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "tokenized_train_datasets.set_format(\"torch\")\n",
    "\n",
    "tokenized_valid_datasets = tokenized_valid_datasets.remove_columns([\"string\"])\n",
    "tokenized_valid_datasets = tokenized_valid_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "tokenized_valid_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfed4c5-9d42-494a-be22-2c32000bb68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(tokenized_train_datasets, shuffle=True, batch_size=8)\n",
    "\n",
    "valid_dataloader = DataLoader(tokenized_valid_datasets, shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768af72-03f3-4d67-ab68-e97af66f22e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device);\n",
    "##NOTE: must be moved prior to optimizer init (https://stackoverflow.com/questions/66091226/runtimeerror-expected-all-tensors-to-be-on-the-same-device-but-found-at-least)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df80d5fe-0ef2-4bc4-9970-e06a67b3a8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(tokenized_train_datasets)\n",
    "print((tokenized_train_datasets['labels'][0]))\n",
    "print(type(tokenized_train_datasets['input_ids'][0]))\n",
    "print(type(tokenized_train_datasets['token_type_ids'][0]))\n",
    "print(type(tokenized_train_datasets['attention_mask'][0]))\n",
    "\n",
    "for i in tokenized_train_datasets['labels']:\n",
    "    print(type(i))\n",
    "    break\n",
    "\n",
    "print(tokenized_train_datasets)\n",
    "print(tokenized_train_datasets['input_ids'].to(device))\n",
    "\n",
    "for i in tokenized_train_datasets['input_ids']:\n",
    "    print(tokenizer.decode(i))\n",
    "    break\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(i).name)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9772b8-be82-49ef-ad45-4f3a95a20f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain = False\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "if retrain:\n",
    "    #tokenized_train_datasets['input_ids'].to(device)\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    loss_values = []\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        \n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            batch['labels'] = batch['labels'].to(device)\n",
    "            batch['input_ids'] = batch['input_ids'].to(device)\n",
    "            batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "            outputs = model(labels = batch['labels'], input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "\n",
    "            #outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "        average_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}, Average Loss: {average_loss}\")\n",
    "\n",
    "        cm = classification_report(true_labels, predicted_labels)\n",
    "        print(\"Classification Report:\")\n",
    "        print(cm)\n",
    "    \"\"\"\n",
    "    plt.plot(range(1, epoch + 1), loss_values, label='Training Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "else:\n",
    "    model = model.from_pretrained(#path to a locally saved T5-large model)\n",
    "    model.to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e4064-888d-491c-8ee0-a6a467e134c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    #Save model\n",
    "    model.save_pretrained(#path to save T5-large model locally, from_pt=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ca958-c5f2-4735-a5cb-e3c1497de656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_references = []\n",
    "all_filenames = []\n",
    "all_input_ids = []\n",
    "all_row_numbers = []\n",
    "\n",
    "for batch in valid_dataloader:\n",
    "    #print(len(batch.items()))\n",
    "    #print((batch.keys()))\n",
    "    #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    #print(k)\n",
    "    #print(v)\n",
    "    \n",
    "    #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #individually move everything BESIDES filename to the gpu\n",
    "        \n",
    "        batch['labels'] = batch['labels'].to(device)\n",
    "        batch['input_ids'] = batch['input_ids'].to(device)\n",
    "        batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "        outputs = model(labels = batch['labels'], input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    clf_metrics.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    all_predictions.extend(predictions.cpu().numpy())\n",
    "    all_references.extend(batch[\"labels\"].cpu().numpy())\n",
    "    all_filenames.extend(batch[\"filename\"])  # Assuming 'filename' is the key in your batch\n",
    "    all_input_ids.extend(batch['input_ids'])\n",
    "    all_row_numbers.extend(batch['Row_Number'])\n",
    "    \n",
    "result_dict = {\n",
    "    \"predictions\": all_predictions,\n",
    "    \"references\": all_references, \n",
    "    \"filenames\": all_filenames, \n",
    "    'input_ids': all_input_ids,\n",
    "    'row_numbers': all_row_numbers\n",
    "}\n",
    "\n",
    "cr = classification_report(all_references,all_predictions, digits = 3)\n",
    "print(cr)\n",
    "\n",
    "\n",
    "clf_metrics.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dcb05e-73cf-43b5-a158-59a1fc31c6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count1 = 0\n",
    "count2 = 0\n",
    "print(len(all_predictions))\n",
    "for i in all_predictions:\n",
    "    if(i == 0):\n",
    "        count1+=1\n",
    "    elif(i == 1):\n",
    "        count2+=1\n",
    "    else:\n",
    "        print(i)\n",
    "print(count1)\n",
    "print(count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2706df-b4cd-46f3-802b-8336fde9e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with filenames, predictions, and references\n",
    "df = pd.DataFrame({\n",
    "    'filename': result_dict[\"filenames\"],\n",
    "    'prediction': result_dict[\"predictions\"],\n",
    "    'reference': result_dict[\"references\"]\n",
    "})\n",
    "\n",
    "# Identify correct and incorrect predictions\n",
    "df['correct'] = (df['prediction'] == df['reference'])\n",
    "\n",
    "\n",
    "# Group by filename and calculate total correct and incorrect predictions\n",
    "summary_df = df.groupby('filename')['correct'].value_counts().unstack(fill_value=0).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "summary_df.columns = ['filename', 'incorrect_predictions',  'correct_predictions']\n",
    "\n",
    "summary_df['total_predicts'] = (summary_df['incorrect_predictions'] + summary_df['correct_predictions'])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b11d87-117c-46a2-a05e-2192fb6d1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sentences = [tokenizer.decode(ids, skip_special_tokens=True) for ids in result_dict['input_ids']]\n",
    "result_dict['decoded_sentences'] = decoded_sentences\n",
    "\n",
    "def export_data(data, path):\n",
    "    f = open(path+\".pkl\",'w')\n",
    "    f.close()\n",
    "    f = open(path+\".pkl\", \"wb\")\n",
    "    pickle.dump(data, f)\n",
    "    f.close()\n",
    "\n",
    "export_data(result_dict, \"PT_T5_Classifier_results_dict_TEST_SET_v1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
